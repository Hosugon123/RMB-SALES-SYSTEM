#!/usr/bin/env python3
"""
ä¸‰åˆä¸€è³‡æ–™ç®¡ç†ç³»çµ±
1. å‚™ä»½ä¿è­· - å®Œæ•´è³‡æ–™å‚™ä»½åˆ° GCS
2. æ¸›å°‘è³‡æ–™åº«å¤§å° - æ­·å²è³‡æ–™æ­¸æª”
3. æ­·å²è³‡æ–™æŸ¥è©¢ - ä¿æŒæŸ¥è©¢èƒ½åŠ›
"""

import os
import sys
import pandas as pd
import psycopg2
from datetime import datetime, timedelta
from google.cloud import storage
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class ArchiveBackupSystem:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # é…ç½®åƒæ•¸
        self.KEEP_MONTHS = 6  # è³‡æ–™åº«ä¿ç•™æœ€è¿‘6å€‹æœˆ
        self.archive_date = datetime.now() - timedelta(days=30 * self.KEEP_MONTHS)
        
        # ç’°å¢ƒè®Šæ•¸
        self.database_url = os.getenv('DATABASE_URL')
        self.gcs_credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        self.gcs_bucket_name = os.getenv('GCS_BUCKET_NAME')
        
        logger.info("=== ä¸‰åˆä¸€è³‡æ–™ç®¡ç†ç³»çµ±åˆå§‹åŒ– ===")
        logger.info(f"ä¿ç•™è³‡æ–™: æœ€è¿‘ {self.KEEP_MONTHS} å€‹æœˆ")
        logger.info(f"æ­¸æª”ç•Œç·š: {self.archive_date.strftime('%Y-%m-%d')}")
        
        # éœ€è¦æ­·å²æ­¸æª”çš„è³‡æ–™è¡¨ï¼ˆå¯é…ç½®ï¼‰
        self.archivable_tables = {
            'sales_records': 'created_at',      # éŠ·å”®è¨˜éŒ„
            'purchase_records': 'created_at',   # æ¡è³¼è¨˜éŒ„
            'cash_logs': 'created_at',          # ç¾é‡‘è¨˜éŒ„
            'ledger_entries': 'created_at',     # åˆ†é¡å¸³æ¢ç›®
            # 'users': None,                    # ç”¨æˆ¶è¡¨ä¸æ­¸æª”
            # 'customers': None,                # å®¢æˆ¶è¡¨ä¸æ­¸æª”
        }

    def connect_database(self):
        """é€£æ¥è³‡æ–™åº«"""
        try:
            self.conn = psycopg2.connect(self.database_url)
            logger.info("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            return False

    def step1_full_backup(self):
        """æ­¥é©Ÿ1: å®Œæ•´å‚™ä»½ä¿è­·"""
        logger.info("ğŸ”’ === æ­¥é©Ÿ1: å®Œæ•´å‚™ä»½ä¿è­· ===")
        
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
            """)
            tables = [row[0] for row in cursor.fetchall()]
            cursor.close()
            
            backup_files = []
            
            for table in tables:
                logger.info(f"å‚™ä»½è³‡æ–™è¡¨: {table}")
                df = pd.read_sql_query(f"SELECT * FROM {table}", self.conn)
                
                if not df.empty:
                    filename = f"full_backup_{table}_{self.timestamp}.xlsx"
                    df.to_excel(filename, index=False, engine='openpyxl')
                    backup_files.append(filename)
                    
                    # ä¸Šå‚³åˆ° GCS
                    gcs_path = f"full_backups/{self.timestamp[:8]}/{filename}"
                    self.upload_to_gcs(filename, gcs_path)
            
            logger.info(f"âœ… å®Œæ•´å‚™ä»½å®Œæˆ: {len(backup_files)} å€‹æª”æ¡ˆ")
            return backup_files
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´å‚™ä»½å¤±æ•—: {str(e)}")
            return []

    def step2_archive_old_data(self):
        """æ­¥é©Ÿ2: æ­·å²è³‡æ–™æ­¸æª”"""
        logger.info("ğŸ“¦ === æ­¥é©Ÿ2: æ­·å²è³‡æ–™æ­¸æª” ===")
        
        archived_files = []
        space_saved = 0
        
        for table, date_column in self.archivable_tables.items():
            if not date_column:
                continue
                
            try:
                logger.info(f"è™•ç†è³‡æ–™è¡¨: {table}")
                
                # æŸ¥è©¢èˆŠè³‡æ–™
                archive_query = f"""
                    SELECT * FROM {table} 
                    WHERE {date_column} < %s
                    ORDER BY {date_column}
                """
                
                df_archive = pd.read_sql_query(
                    archive_query, 
                    self.conn, 
                    params=[self.archive_date]
                )
                
                if df_archive.empty:
                    logger.info(f"âšª {table}: æ²’æœ‰éœ€è¦æ­¸æª”çš„è³‡æ–™")
                    continue
                
                # ä¿å­˜æ­·å²è³‡æ–™åˆ° Excel
                archive_filename = f"archive_{table}_{self.timestamp}.xlsx"
                df_archive.to_excel(archive_filename, index=False, engine='openpyxl')
                archived_files.append(archive_filename)
                
                # ä¸Šå‚³æ­·å²è³‡æ–™åˆ° GCS
                gcs_path = f"archives/{self.timestamp[:6]}/{archive_filename}"
                self.upload_to_gcs(archive_filename, gcs_path)
                
                # è¨ˆç®—è¦åˆªé™¤çš„è¨˜éŒ„æ•¸
                record_count = len(df_archive)
                
                # åˆªé™¤è³‡æ–™åº«ä¸­çš„èˆŠè³‡æ–™
                cursor = self.conn.cursor()
                delete_query = f"DELETE FROM {table} WHERE {date_column} < %s"
                cursor.execute(delete_query, [self.archive_date])
                deleted_count = cursor.rowcount
                self.conn.commit()
                cursor.close()
                
                space_saved += deleted_count
                
                logger.info(f"âœ… {table}: æ­¸æª” {record_count} ç­†ï¼Œåˆªé™¤ {deleted_count} ç­†")
                
            except Exception as e:
                logger.error(f"âŒ æ­¸æª” {table} å¤±æ•—: {str(e)}")
                self.conn.rollback()
        
        logger.info(f"ğŸ“‰ ç¸½è¨ˆç¯€çœè³‡æ–™åº«ç©ºé–“: ç´„ {space_saved} ç­†è¨˜éŒ„")
        return archived_files

    def step3_create_query_index(self):
        """æ­¥é©Ÿ3: å‰µå»ºæ­·å²è³‡æ–™æŸ¥è©¢ç´¢å¼•"""
        logger.info("ğŸ” === æ­¥é©Ÿ3: æ­·å²è³‡æ–™æŸ¥è©¢ç´¢å¼• ===")
        
        try:
            # å‰µå»ºæŸ¥è©¢ç´¢å¼•
            query_index = {
                'å‚™ä»½æ—¥æœŸ': self.timestamp[:8],
                'æ­¸æª”ç•Œç·š': self.archive_date.strftime('%Y-%m-%d'),
                'ä¿ç•™æ”¿ç­–': f'æœ€è¿‘ {self.KEEP_MONTHS} å€‹æœˆ',
                'æ­¸æª”ä½ç½®': f'gs://{self.gcs_bucket_name}/archives/{self.timestamp[:6]}/',
                'å®Œæ•´å‚™ä»½ä½ç½®': f'gs://{self.gcs_bucket_name}/full_backups/{self.timestamp[:8]}/',
                'æŸ¥è©¢èªªæ˜': 'æ­·å²è³‡æ–™å¯å¾ GCS archives è³‡æ–™å¤¾ä¸‹è¼‰æŸ¥è©¢'
            }
            
            # çµ±è¨ˆç•¶å‰è³‡æ–™åº«ç‹€æ…‹
            cursor = self.conn.cursor()
            db_stats = []
            
            for table in self.archivable_tables.keys():
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                current_count = cursor.fetchone()[0]
                
                db_stats.append({
                    'è³‡æ–™è¡¨': table,
                    'ç•¶å‰è¨˜éŒ„æ•¸': current_count,
                    'ç‹€æ…‹': 'å·²æ­¸æª”' if current_count > 0 else 'ç©º'
                })
            
            cursor.close()
            
            # å‰µå»ºæŸ¥è©¢æŒ‡å— Excel
            with pd.ExcelWriter(f"query_index_{self.timestamp}.xlsx", engine='openpyxl') as writer:
                # åŸºæœ¬è³‡è¨Š
                pd.DataFrame([query_index]).to_excel(writer, sheet_name='åŸºæœ¬è³‡è¨Š', index=False)
                
                # è³‡æ–™åº«ç‹€æ…‹
                pd.DataFrame(db_stats).to_excel(writer, sheet_name='è³‡æ–™åº«ç‹€æ…‹', index=False)
                
                # æŸ¥è©¢èªªæ˜
                query_guide = pd.DataFrame([
                    {'æ­¥é©Ÿ': 1, 'èªªæ˜': 'æŸ¥è©¢æœ€è¿‘6å€‹æœˆè³‡æ–™ï¼šç›´æ¥ä½¿ç”¨ä¸»è³‡æ–™åº«'},
                    {'æ­¥é©Ÿ': 2, 'èªªæ˜': 'æŸ¥è©¢æ­·å²è³‡æ–™ï¼šå¾ GCS archives ä¸‹è¼‰å°æ‡‰æª”æ¡ˆ'},
                    {'æ­¥é©Ÿ': 3, 'èªªæ˜': 'å®Œæ•´è³‡æ–™æŸ¥è©¢ï¼šçµåˆè³‡æ–™åº« + GCS æ­·å²æª”æ¡ˆ'},
                    {'æ­¥é©Ÿ': 4, 'èªªæ˜': 'ç·Šæ€¥æ¢å¾©ï¼šä½¿ç”¨ full_backups å®Œæ•´å‚™ä»½'}
                ])
                query_guide.to_excel(writer, sheet_name='æŸ¥è©¢æŒ‡å—', index=False)
            
            # ä¸Šå‚³æŸ¥è©¢ç´¢å¼•
            index_filename = f"query_index_{self.timestamp}.xlsx"
            gcs_path = f"query_indexes/{index_filename}"
            self.upload_to_gcs(index_filename, gcs_path)
            
            logger.info("âœ… æ­·å²è³‡æ–™æŸ¥è©¢ç´¢å¼•å‰µå»ºå®Œæˆ")
            return index_filename
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºæŸ¥è©¢ç´¢å¼•å¤±æ•—: {str(e)}")
            return None

    def upload_to_gcs(self, local_file, gcs_path):
        """ä¸Šå‚³åˆ° GCS"""
        try:
            client = storage.Client.from_service_account_json(self.gcs_credentials_path)
            bucket = client.bucket(self.gcs_bucket_name)
            blob = bucket.blob(gcs_path)
            
            blob.upload_from_filename(local_file)
            blob.metadata = {
                'source': 'archive-backup-system',
                'timestamp': self.timestamp,
                'type': gcs_path.split('/')[0]
            }
            blob.patch()
            
            return True
        except Exception as e:
            logger.error(f"âŒ ä¸Šå‚³å¤±æ•— {local_file}: {str(e)}")
            return False

    def cleanup_local_files(self, files):
        """æ¸…ç†æœ¬åœ°æª”æ¡ˆ"""
        total_size = 0
        for file in files:
            if os.path.exists(file):
                size = os.path.getsize(file)
                total_size += size
                os.remove(file)
                logger.info(f"ğŸ§¹ å·²æ¸…ç†: {file} ({size/1024:.1f} KB)")
        
        logger.info(f"ğŸ’¾ ç¸½å…±ç¯€çœæœ¬åœ°ç©ºé–“: {total_size/1024:.1f} KB")

    def run_complete_system(self):
        """åŸ·è¡Œå®Œæ•´çš„ä¸‰åˆä¸€ç³»çµ±"""
        all_files = []
        
        try:
            logger.info("ğŸš€ === ä¸‰åˆä¸€è³‡æ–™ç®¡ç†ç³»çµ±é–‹å§‹ ===")
            
            # é€£æ¥è³‡æ–™åº«
            if not self.connect_database():
                raise Exception("è³‡æ–™åº«é€£æ¥å¤±æ•—")
            
            # æ­¥é©Ÿ1: å®Œæ•´å‚™ä»½ä¿è­·
            backup_files = self.step1_full_backup()
            all_files.extend(backup_files)
            
            # æ­¥é©Ÿ2: æ­·å²è³‡æ–™æ­¸æª”ï¼ˆæ¸›å°‘è³‡æ–™åº«å¤§å°ï¼‰
            archive_files = self.step2_archive_old_data()
            all_files.extend(archive_files)
            
            # æ­¥é©Ÿ3: å‰µå»ºæŸ¥è©¢ç´¢å¼•
            index_file = self.step3_create_query_index()
            if index_file:
                all_files.append(index_file)
            
            self.conn.close()
            
            logger.info("ğŸ‰ === ä¸‰åˆä¸€ç³»çµ±åŸ·è¡Œå®Œæˆ ===")
            logger.info("âœ… å‚™ä»½ä¿è­·: å®Œæ•´è³‡æ–™å·²å‚™ä»½")
            logger.info("ğŸ“‰ è³‡æ–™åº«å¤§å°: æ­·å²è³‡æ–™å·²æ­¸æª”")
            logger.info("ğŸ” æ­·å²æŸ¥è©¢: æŸ¥è©¢ç´¢å¼•å·²å»ºç«‹")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±åŸ·è¡Œå¤±æ•—: {str(e)}")
            return False
            
        finally:
            # æ¸…ç†æœ¬åœ°æª”æ¡ˆ
            if all_files:
                self.cleanup_local_files(all_files)

def main():
    try:
        system = ArchiveBackupSystem()
        success = system.run_complete_system()
        
        if success:
            logger.info("âœ… ä¸‰åˆä¸€ç³»çµ±åŸ·è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("âŒ ä¸‰åˆä¸€ç³»çµ±åŸ·è¡Œå¤±æ•—")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼ç•°å¸¸: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
