#!/usr/bin/env python3
"""
PostgreSQL è³‡æ–™åº«å‚™ä»½è…³æœ¬
å¾ Render PostgreSQL è³‡æ–™åº«å‚™ä»½è³‡æ–™åˆ° Google Cloud Storage
"""

import os
import sys
import pandas as pd
import psycopg2
from datetime import datetime, timedelta
from google.cloud import storage
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class DatabaseBackup:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
        self.database_url = os.getenv('DATABASE_URL')
        self.gcs_credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        self.gcs_bucket_name = os.getenv('GCS_BUCKET_NAME')
        
        logger.info("=== è³‡æ–™åº«å‚™ä»½åˆå§‹åŒ– ===")
        logger.info(f"æ™‚é–“æˆ³: {self.timestamp}")
        logger.info(f"GCS å„²å­˜æ¡¶: {self.gcs_bucket_name}")
        
        # é©—è­‰ç’°å¢ƒè®Šæ•¸
        if not self.database_url:
            raise ValueError("DATABASE_URL ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")
        if not self.gcs_credentials_path:
            raise ValueError("GOOGLE_APPLICATION_CREDENTIALS ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")
        if not self.gcs_bucket_name:
            raise ValueError("GCS_BUCKET_NAME ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")

    def connect_database(self):
        """é€£æ¥åˆ° PostgreSQL è³‡æ–™åº«"""
        try:
            logger.info("æ­£åœ¨é€£æ¥åˆ° PostgreSQL è³‡æ–™åº«...")
            self.conn = psycopg2.connect(self.database_url)
            logger.info("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            return False

    def get_all_tables(self):
        """ç²å–æ‰€æœ‰è³‡æ–™è¡¨"""
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """)
            tables = [row[0] for row in cursor.fetchall()]
            cursor.close()
            
            logger.info(f"æ‰¾åˆ° {len(tables)} å€‹è³‡æ–™è¡¨: {', '.join(tables)}")
            return tables
        except Exception as e:
            logger.error(f"âŒ ç²å–è³‡æ–™è¡¨æ¸…å–®å¤±æ•—: {str(e)}")
            return []

    def backup_table_to_excel(self, table_name):
        """å‚™ä»½å–®å€‹è³‡æ–™è¡¨åˆ° Excel"""
        try:
            logger.info(f"æ­£åœ¨å‚™ä»½è³‡æ–™è¡¨: {table_name}")
            
            # æŸ¥è©¢è³‡æ–™è¡¨å…§å®¹
            df = pd.read_sql_query(f"SELECT * FROM {table_name}", self.conn)
            
            if df.empty:
                logger.info(f"âš ï¸ è³‡æ–™è¡¨ {table_name} ç‚ºç©ºï¼Œè·³éå‚™ä»½")
                return None
            
            # ç”Ÿæˆæª”æ¡ˆå
            filename = f"{table_name}_backup_{self.timestamp}.xlsx"
            
            # ä¿å­˜ç‚º Excel
            df.to_excel(filename, index=False, engine='openpyxl')
            
            logger.info(f"âœ… {table_name} å‚™ä»½å®Œæˆ: {len(df)} ç­†è¨˜éŒ„ -> {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½è³‡æ–™è¡¨ {table_name} å¤±æ•—: {str(e)}")
            return None

    def create_summary_report(self, backup_results):
        """å‰µå»ºå‚™ä»½æ‘˜è¦å ±å‘Š"""
        try:
            logger.info("æ­£åœ¨å‰µå»ºå‚™ä»½æ‘˜è¦å ±å‘Š...")
            
            # çµ±è¨ˆè³‡è¨Š
            summary_data = []
            total_records = 0
            
            for table_name, result in backup_results.items():
                if result['success']:
                    # é‡æ–°æŸ¥è©¢çµ±è¨ˆè³‡è¨Š
                    cursor = self.conn.cursor()
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    cursor.close()
                    
                    summary_data.append({
                        'è³‡æ–™è¡¨åç¨±': table_name,
                        'è¨˜éŒ„æ•¸é‡': count,
                        'å‚™ä»½ç‹€æ…‹': 'æˆåŠŸ',
                        'å‚™ä»½æª”æ¡ˆ': result['filename'],
                        'å‚™ä»½æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    total_records += count
                else:
                    summary_data.append({
                        'è³‡æ–™è¡¨åç¨±': table_name,
                        'è¨˜éŒ„æ•¸é‡': 0,
                        'å‚™ä»½ç‹€æ…‹': 'å¤±æ•—',
                        'å‚™ä»½æª”æ¡ˆ': 'ç„¡',
                        'å‚™ä»½æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
            
            # æ·»åŠ ç¸½è¨ˆè¡Œ
            summary_data.append({
                'è³‡æ–™è¡¨åç¨±': '=== ç¸½è¨ˆ ===',
                'è¨˜éŒ„æ•¸é‡': total_records,
                'å‚™ä»½ç‹€æ…‹': f"{len([r for r in backup_results.values() if r['success']])}/{len(backup_results)} æˆåŠŸ",
                'å‚™ä»½æª”æ¡ˆ': f"å…± {len([r for r in backup_results.values() if r['success']])} å€‹æª”æ¡ˆ",
                'å‚™ä»½æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            
            # å‰µå»ºæ‘˜è¦ Excel
            df_summary = pd.DataFrame(summary_data)
            summary_filename = f"backup_summary_{self.timestamp}.xlsx"
            df_summary.to_excel(summary_filename, index=False, engine='openpyxl')
            
            logger.info(f"âœ… å‚™ä»½æ‘˜è¦å ±å‘Šå‰µå»ºå®Œæˆ: {summary_filename}")
            logger.info(f"ğŸ“Š ç¸½å…±å‚™ä»½ {total_records} ç­†è¨˜éŒ„")
            
            return summary_filename
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºæ‘˜è¦å ±å‘Šå¤±æ•—: {str(e)}")
            return None

    def upload_to_gcs(self, local_file, gcs_path):
        """ä¸Šå‚³æª”æ¡ˆåˆ° GCS"""
        try:
            client = storage.Client.from_service_account_json(self.gcs_credentials_path)
            bucket = client.bucket(self.gcs_bucket_name)
            blob = bucket.blob(gcs_path)
            
            blob.upload_from_filename(local_file)
            
            # è¨­ç½®å…ƒæ•¸æ“š
            blob.metadata = {
                'source': 'database-backup',
                'timestamp': self.timestamp,
                'backup_type': 'postgresql'
            }
            blob.patch()
            
            logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸ: gs://{self.gcs_bucket_name}/{gcs_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æª”æ¡ˆä¸Šå‚³å¤±æ•—: {str(e)}")
            return False

    def cleanup_local_files(self, files):
        """æ¸…ç†æœ¬åœ°æª”æ¡ˆ"""
        try:
            for file in files:
                if os.path.exists(file):
                    os.remove(file)
                    logger.info(f"ğŸ§¹ å·²æ¸…ç†: {file}")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æª”æ¡ˆå¤±æ•—: {str(e)}")

    def run_backup(self):
        """åŸ·è¡Œå®Œæ•´å‚™ä»½æµç¨‹"""
        local_files = []
        
        try:
            logger.info("ğŸš€ === é–‹å§‹è³‡æ–™åº«å‚™ä»½ ===")
            
            # 1. é€£æ¥è³‡æ–™åº«
            if not self.connect_database():
                raise Exception("è³‡æ–™åº«é€£æ¥å¤±æ•—")
            
            # 2. ç²å–æ‰€æœ‰è³‡æ–™è¡¨
            tables = self.get_all_tables()
            if not tables:
                raise Exception("æ²’æœ‰æ‰¾åˆ°è³‡æ–™è¡¨")
            
            # 3. å‚™ä»½æ¯å€‹è³‡æ–™è¡¨
            backup_results = {}
            
            for table in tables:
                filename = self.backup_table_to_excel(table)
                if filename:
                    backup_results[table] = {
                        'success': True,
                        'filename': filename
                    }
                    local_files.append(filename)
                else:
                    backup_results[table] = {
                        'success': False,
                        'filename': None
                    }
            
            # 4. å‰µå»ºæ‘˜è¦å ±å‘Š
            summary_file = self.create_summary_report(backup_results)
            if summary_file:
                local_files.append(summary_file)
            
            # 5. ä¸Šå‚³æ‰€æœ‰æª”æ¡ˆåˆ° GCS
            upload_success = 0
            for file in local_files:
                gcs_path = f"database_backups/{self.timestamp[:8]}/{file}"
                if self.upload_to_gcs(file, gcs_path):
                    upload_success += 1
            
            logger.info(f"ğŸ“¤ æˆåŠŸä¸Šå‚³ {upload_success}/{len(local_files)} å€‹æª”æ¡ˆ")
            
            # é—œé–‰è³‡æ–™åº«é€£æ¥
            self.conn.close()
            logger.info("è³‡æ–™åº«é€£æ¥å·²é—œé–‰")
            
            logger.info("ğŸ‰ === è³‡æ–™åº«å‚™ä»½å®Œæˆ ===")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å‚™ä»½å¤±æ•—: {str(e)}")
            return False
            
        finally:
            # æ¸…ç†æœ¬åœ°æª”æ¡ˆ
            if local_files:
                self.cleanup_local_files(local_files)

def main():
    try:
        backup = DatabaseBackup()
        success = backup.run_backup()
        
        if success:
            logger.info("âœ… å‚™ä»½ç¨‹å¼åŸ·è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("âŒ å‚™ä»½ç¨‹å¼åŸ·è¡Œå¤±æ•—")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼ç•°å¸¸: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
