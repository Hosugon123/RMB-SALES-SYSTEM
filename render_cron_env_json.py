#!/usr/bin/env python3
"""
Render Cron Job - ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ JSON èªè­‰
é¿å… Secret Files è·¯å¾‘å•é¡Œ
"""

import os
import sys
import pandas as pd
import json
import tempfile
from datetime import datetime
from google.cloud import storage
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class RenderCronJob:
    def __init__(self):
        """åˆå§‹åŒ– Cron Job - ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ JSON"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
        self.gcs_credentials_json = os.getenv('GCS_CREDENTIALS_JSON')
        self.gcs_bucket_name = os.getenv('GCS_BUCKET_NAME')
        self.target_url = os.getenv('TARGET_URL', 'https://www.google.com')
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆ - æ™‚é–“æˆ³: {self.timestamp}")
        logger.info(f"GCS å„²å­˜æ¡¶: {self.gcs_bucket_name}")
        
        # æª¢æŸ¥å¿…è¦çš„ç’°å¢ƒè®Šæ•¸
        if not self.gcs_credentials_json:
            raise ValueError("GCS_CREDENTIALS_JSON ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")
        if not self.gcs_bucket_name:
            raise ValueError("GCS_BUCKET_NAME ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")

    def create_gcs_client(self):
        """å‰µå»º GCS å®¢æˆ¶ç«¯ - ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ JSON"""
        try:
            logger.info("å‰µå»º GCS å®¢æˆ¶ç«¯...")
            
            # è§£æ JSON èªè­‰
            creds_data = json.loads(self.gcs_credentials_json)
            
            # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
                json.dump(creds_data, temp_file)
                temp_path = temp_file.name
            
            # å‰µå»ºå®¢æˆ¶ç«¯
            client = storage.Client.from_service_account_json(temp_path)
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            os.unlink(temp_path)
            
            logger.info("âœ… GCS å®¢æˆ¶ç«¯å‰µå»ºæˆåŠŸ")
            return client
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON æ ¼å¼éŒ¯èª¤: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"âŒ å‰µå»º GCS å®¢æˆ¶ç«¯å¤±æ•—: {str(e)}")
            raise

    def test_gcs_connection(self):
        """æ¸¬è©¦ GCS é€£ç·š"""
        try:
            logger.info("=== æ¸¬è©¦ GCS é€£ç·š ===")
            
            client = self.create_gcs_client()
            bucket = client.bucket(self.gcs_bucket_name)
            
            # æ¸¬è©¦å„²å­˜æ¡¶æ˜¯å¦å­˜åœ¨
            if bucket.exists():
                logger.info(f"âœ… å„²å­˜æ¡¶ '{self.gcs_bucket_name}' é€£ç·šæˆåŠŸ")
                
                # åˆ—å‡ºç¾æœ‰æª”æ¡ˆ
                blobs = list(bucket.list_blobs(max_results=5))
                logger.info(f"ğŸ“Š å„²å­˜æ¡¶ä¸­æœ‰ {len(blobs)} å€‹æª”æ¡ˆ")
                
                if blobs:
                    logger.info("æœ€è¿‘çš„æª”æ¡ˆ:")
                    for blob in blobs[:3]:
                        logger.info(f"  - {blob.name} ({blob.time_created})")
                
                return True
            else:
                logger.error(f"âŒ å„²å­˜æ¡¶ '{self.gcs_bucket_name}' ä¸å­˜åœ¨")
                return False
                
        except Exception as e:
            logger.error(f"âŒ GCS é€£ç·šæ¸¬è©¦å¤±æ•—: {str(e)}")
            return False

    def generate_backup_excel(self):
        """ç”Ÿæˆå‚™ä»½ Excel æª”æ¡ˆ"""
        try:
            logger.info("=== ç”Ÿæˆå‚™ä»½ Excel æª”æ¡ˆ ===")
            
            # å‰µå»ºå‚™ä»½æ•¸æ“š
            data = {
                'å‚™ä»½æ™‚é–“': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 5,
                'å‚™ä»½é¡å‹': ['è‡ªå‹•å‚™ä»½'] * 5,
                'ç‹€æ…‹': ['æˆåŠŸ'] * 5,
                'æª”æ¡ˆå¤§å°(KB)': [100 + i*50 for i in range(5)],
                'å‚™è¨»': [f'å‚™ä»½é …ç›® {i+1}' for i in range(5)]
            }
            
            df = pd.DataFrame(data)
            filename = f"backup_report_{self.timestamp}.xlsx"
            df.to_excel(filename, index=False, engine='openpyxl')
            
            logger.info(f"âœ… Excel æª”æ¡ˆç”ŸæˆæˆåŠŸ: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ Excel æª”æ¡ˆç”Ÿæˆå¤±æ•—: {str(e)}")
            return None

    def upload_to_gcs(self, local_file, gcs_path):
        """ä¸Šå‚³æª”æ¡ˆåˆ° GCS"""
        try:
            logger.info(f"=== ä¸Šå‚³æª”æ¡ˆåˆ° GCS ===")
            logger.info(f"æœ¬åœ°æª”æ¡ˆ: {local_file}")
            logger.info(f"GCS è·¯å¾‘: {gcs_path}")
            
            client = self.create_gcs_client()
            bucket = client.bucket(self.gcs_bucket_name)
            blob = bucket.blob(gcs_path)
            
            # ä¸Šå‚³æª”æ¡ˆ
            blob.upload_from_filename(local_file)
            
            # è¨­ç½®æª”æ¡ˆå…ƒæ•¸æ“š
            blob.metadata = {
                'source': 'render-cron-job',
                'timestamp': self.timestamp,
                'backup_type': 'automated'
            }
            blob.patch()
            
            logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸ: gs://{self.gcs_bucket_name}/{gcs_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æª”æ¡ˆä¸Šå‚³å¤±æ•—: {str(e)}")
            return False

    def cleanup_local_files(self, file_paths):
        """æ¸…ç†æœ¬åœ°æª”æ¡ˆ"""
        try:
            logger.info("=== æ¸…ç†æœ¬åœ°æª”æ¡ˆ ===")
            
            for file_path in file_paths:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.info(f"âœ… å·²åˆªé™¤: {file_path}")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æª”æ¡ˆå¤±æ•—: {str(e)}")

    def run(self):
        """åŸ·è¡Œä¸»è¦çš„ Cron Job æµç¨‹"""
        local_files = []
        
        try:
            logger.info("ğŸš€ === é–‹å§‹åŸ·è¡Œ Render Cron Job ===")
            
            # 1. æ¸¬è©¦ GCS é€£ç·š
            if not self.test_gcs_connection():
                raise Exception("GCS é€£ç·šæ¸¬è©¦å¤±æ•—")
            
            # 2. ç”Ÿæˆå‚™ä»½æª”æ¡ˆ
            excel_file = self.generate_backup_excel()
            if not excel_file:
                raise Exception("å‚™ä»½æª”æ¡ˆç”Ÿæˆå¤±æ•—")
            
            local_files.append(excel_file)
            
            # 3. ä¸Šå‚³åˆ° GCS
            gcs_path = f"backups/{excel_file}"
            if not self.upload_to_gcs(excel_file, gcs_path):
                raise Exception("æª”æ¡ˆä¸Šå‚³å¤±æ•—")
            
            logger.info("ğŸ‰ === Cron Job åŸ·è¡ŒæˆåŠŸ ===")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Cron Job åŸ·è¡Œå¤±æ•—: {str(e)}")
            return False
            
        finally:
            # æ¸…ç†æœ¬åœ°æª”æ¡ˆ
            if local_files:
                self.cleanup_local_files(local_files)

def main():
    """ä¸»å‡½æ•¸"""
    try:
        cron_job = RenderCronJob()
        success = cron_job.run()
        
        if success:
            logger.info("âœ… ç¨‹å¼åŸ·è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼ç•°å¸¸: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
